# -*- coding: utf-8 -*-
"""제조 AI Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caYTnfuwIsV06-PNlv6g4hoEf48BJ8Ra
"""

# =========================
# 0. 사전학습 영역
# =========================

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import pickle
import json

# =========================
# 1. 정상 데이터 로드
# =========================

print("Loading normal.csv...")
df = pd.read_csv("normal.csv")

# Fault Number 컬럼이 마지막에 있다면 제거
# (일부 normal.csv에는 없을 수도 있으니 자동 체크)
# 숫자형 센서만 추출
df_features = df.select_dtypes(include=[np.number]).copy()

# NaN 제거
df_features = df_features.fillna(method="ffill").fillna(method="bfill")

print("Data shape:", df_features.shape)

# =========================
# 2. 정규화 (Scaler 학습)
# =========================

print("Fitting StandardScaler...")
scaler = StandardScaler()
scaled = scaler.fit_transform(df_features)

# =========================
# 3. PCA 학습
# =========================

print("Fitting PCA (90% explained variance)...")
pca = PCA(n_components=0.90)
pca.fit(scaled)

print(f"PCA components learned: {pca.n_components_}")

# =========================
# 4. 정상 데이터 기반 threshold 자동 계산 (Hotelling T²)
# =========================

# SPE 추가 개시

# =========================
# 4-2. SPE(Q-statistics) Threshold 계산
# =========================

# reconstruction 기반 SPE 계산 함수
def compute_spe(pca, scaled_x):
    x_pca = pca.transform(scaled_x)
    x_recon = pca.inverse_transform(x_pca)
    residual = scaled_x - x_recon
    spe = np.sum(residual**2)
    return float(spe)

# 정상 데이터 SPE 전체 계산
spe_scores = []
for i in range(len(scaled)):
    spe_scores.append(compute_spe(pca, scaled[i:i+1]))

spe_threshold = np.percentile(spe_scores, 99)

print("SPE Threshold:", spe_threshold)

# SPE threshold 따로 저장
with open("threshold_spe.txt", "w") as f:
    f.write(str(spe_threshold))


# SPE 추가 끝

print("Computing threshold...")

def compute_risk_pca(pca, scaled_x):
    x_pca = pca.transform(scaled_x)
    lambdas = pca.explained_variance_
    t2 = np.sum((x_pca[0]**2) / lambdas)
    return float(t2)

# 모든 정상 데이터에 대해 T² 점수 계산
scores = []
for i in range(len(scaled)):
    scores.append(compute_risk_pca(pca, scaled[i:i+1]))

threshold = np.percentile(scores, 95)
print("Threshold :", threshold)

# =========================
# 5. 모델 저장
# =========================

print("Saving scaler.pkl and pca.pkl ...")

with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

with open("pca.pkl", "wb") as f:
    pickle.dump(pca, f)

with open("threshold.txt", "w") as f:
    f.write(str(threshold))

print("Training complete!")
print("Generated files: scaler.pkl, pca.pkl, threshold.txt")

# =========================
# 0. 기본 프레임 준비
# =========================

class EventLog:

    def __init__(self):
        self.logs = []

    def add(self, event_dict: dict):
        self.logs.append(event_dict)

# 실제 모델 로드
import pickle

with open("scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open("pca.pkl", "rb") as f:
    pca = pickle.load(f)








# =========================
# 1. Warn Loop (프레임)
# =========================

import time
import random
import threading
import numpy as np
from collections import deque


df = pd.read_csv("test2.csv")


history_buffer = deque(maxlen=5)   # 최근 30개 snapshot 저장

sensor_cols = df.select_dtypes(include=[np.number]).values   # 숫자만 남김
timestamps  = df.iloc[:, 0].values  # 첫 컬럼 timestamp 저장

cursor = 0
def get_latest_snapshot():
    global cursor
    snap = sensor_cols[cursor]   # 숫자 52개만
    cursor += 1
    return snap

def compute_risk_pca(pca, scaled_x):
    x_pca = pca.transform(scaled_x)
    lambdas = pca.explained_variance_
    t2 = np.sum((x_pca[0]**2) / lambdas)
    return float(t2)


# 1) feature별 기여도 계산
def get_feature_contributions(pca, scaled_x):
    x_pca = pca.transform(scaled_x)[0]      # (n_components,)
    loadings = pca.components_.T            # (n_features, n_components)

    raw_contrib = np.abs(loadings * x_pca).sum(axis=1)  # (n_features,)
    norm_contrib = raw_contrib / raw_contrib.sum()      # 0~1 정규화

    return norm_contrib

# SPE feature contribution 계산
def get_spe_contributions(pca, scaled_x):
    x_pca = pca.transform(scaled_x)
    x_recon = pca.inverse_transform(x_pca)
    residual = (scaled_x - x_recon)[0]  # (n_features,)

    raw = residual**2
    norm = raw / raw.sum()

    return norm

def get_top3_spe_features(pca, scaled_x):
    contrib = get_spe_contributions(pca, scaled_x)
    top3_idx = contrib.argsort()[-3:][::-1]

    result = []
    for idx in top3_idx:
        result.append({
            "sensor": int(idx+1),
            "score": float(contrib[idx])
        })
    return result


# 2) Top-3 + 기여도(score)까지 반환
def get_top3_features_with_scores(pca, scaled_x):
    contrib = get_feature_contributions(pca, scaled_x)
    top3_idx = contrib.argsort()[-3:][::-1]   # 큰 순서대로 3개

    result = []
    for idx in top3_idx:
        result.append({
            "sensor": int(idx+1),
            "score": float(contrib[idx])
        })

    return result

def warn_loop(scaler, pca, log: EventLog, threshold_t2, threshold_spe):
    while True:
        snap = get_latest_snapshot()
        history_buffer.append(snap.tolist())
        snap_scaled = scaler.transform(snap.reshape(1, -1))



        risk_t2  = compute_risk_pca(pca, snap_scaled)
        risk_spe = compute_spe(pca, snap_scaled)

        print(f"[T2] {risk_t2:.4f}   [SPE] {risk_spe:.4f}")


        if (risk_t2 > threshold_t2) or (risk_spe > threshold_spe):
            top3_t2  = get_top3_features_with_scores(pca, snap_scaled)
            top3_spe = get_top3_spe_features(pca, snap_scaled)

            event = {
                "event_type": "WARN",
                "timestamp": time.time(),
                "risk": float(risk_t2),
                "spe": float(risk_spe),
                "top3_t2": top3_t2,
                "top3_spe": top3_spe,
                "history": list(history_buffer),
                "alarm_code": None,
                "raw_data": snap.tolist(),
                "source": "sensor"
            }
            log.add(event)

        time.sleep(3)


# =========================
# 2. Alarm Trigger (프레임)
# =========================

# ===== Alarm 시 PCA 분석 함수 =====
def analyze_alarm_snapshot(pca, scaler, history_buffer):
    # 최신 스냅 1개
    snap = np.array(history_buffer[-1]).reshape(1, -1)
    snap_scaled = scaler.transform(snap)

    # T², SPE 계산
    t2 = compute_risk_pca(pca, snap_scaled)
    spe = compute_spe(pca, snap_scaled)

    # Top-3 기여 센서
    top3_t2  = get_top3_features_with_scores(pca, snap_scaled)
    top3_spe = get_top3_spe_features(pca, snap_scaled)

    return {
        "risk": float(t2),
        "spe": float(spe),
        "top3_t2": top3_t2,
        "top3_spe": top3_spe
    }


def trigger_alarm(code, log: EventLog):
    global history_buffer

    latest_snap = np.array(history_buffer[-1]).reshape(1, -1)
    analysis = analyze_alarm_snapshot(pca, scaler, history_buffer)

    event = {
        "event_type": "ALARM",
        "timestamp": time.time(),

        "risk": analysis["risk"],
        "spe": analysis["spe"],
        "top3_t2": analysis["top3_t2"],
        "top3_spe": analysis["top3_spe"],
        "history": list(history_buffer),

        "alarm_code": code,
        "raw_data": latest_snap.tolist(),
        "source": "machine"
    }

    event = json.loads(json.dumps(event, default=float))
    log.add(event)




# =========================
# 3. 메인 구동부
# =========================

log = EventLog()


threshold = float(open("threshold.txt").read())   # 임시값, 나중에 정상데이터 기반으로 계산된 threshold 넣으면 됨
spe_threshold = float(open("threshold_spe.txt").read())


warn_thread = threading.Thread(target=warn_loop, args=(scaler, pca, log, threshold, spe_threshold))
warn_thread.daemon = True
warn_thread.start()

'''
time.sleep(60)
log.logs[:3]
'''

'''
time.sleep(5)  # warn_loop 조금 돌리고

print(">>> ALARM TEST RUN")
trigger_alarm(code=101, log=log)

print(">>> LAST LOG")
print(log.logs[-1])
'''

